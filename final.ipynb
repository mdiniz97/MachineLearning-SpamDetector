{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cfedde3728e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat(\"spam_data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'names', 'P_train', 'T_train', 'P_test', 'T_test'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for i in range(len(data['names'])):\n",
    "    names.append(data['names'][i][1][0])\n",
    "\n",
    "X = np.concatenate([data['P_train'].transpose(),data['P_test'].transpose()])\n",
    "y = np.concatenate([data['T_train'].transpose(),data['T_test'].transpose()]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf = pd.DataFrame(X, columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>3d</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>move</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>...</th>\n",
       "      <th>conference</th>\n",
       "      <th>;</th>\n",
       "      <th>(</th>\n",
       "      <th>[</th>\n",
       "      <th>!</th>\n",
       "      <th>$</th>\n",
       "      <th>#</th>\n",
       "      <th>capAve</th>\n",
       "      <th>capLong</th>\n",
       "      <th>capTot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61.0</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485.0</td>\n",
       "      <td>2259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40.0</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40.0</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   make  address   all   3d   our  over  move  internet  order  mail   ...    \\\n",
       "0  0.00     0.64  0.64  0.0  0.32  0.00  0.00      0.00   0.00  0.00   ...     \n",
       "1  0.06     0.00  0.71  0.0  1.23  0.19  0.19      0.12   0.64  0.25   ...     \n",
       "2  0.00     0.00  0.00  0.0  0.63  0.00  0.31      0.63   0.31  0.63   ...     \n",
       "3  0.00     0.00  0.00  0.0  0.63  0.00  0.31      0.63   0.31  0.63   ...     \n",
       "4  0.00     0.00  0.00  0.0  1.85  0.00  0.00      1.85   0.00  0.00   ...     \n",
       "\n",
       "   conference     ;      (    [      !      $     #  capAve  capLong  capTot  \n",
       "0         0.0  0.00  0.000  0.0  0.778  0.000  0.00   3.756     61.0   278.0  \n",
       "1         0.0  0.01  0.143  0.0  0.276  0.184  0.01   9.821    485.0  2259.0  \n",
       "2         0.0  0.00  0.137  0.0  0.137  0.000  0.00   3.537     40.0   191.0  \n",
       "3         0.0  0.00  0.135  0.0  0.135  0.000  0.00   3.537     40.0   191.0  \n",
       "4         0.0  0.00  0.223  0.0  0.000  0.000  0.00   3.000     15.0    54.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf = pd.DataFrame(y, columns = ['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class\n",
       "0     -1\n",
       "1     -1\n",
       "2     -1\n",
       "3     -1\n",
       "4     -1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataDf, np.ravel(testDf), test_size=0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\"activation\":[\"relu\", \"tanh\", \"identity\", \"logistic\"], \"solver\" : ['lbfgs', 'sgd', 'adam'],\n",
    "              \"alpha\": np.linspace(0.00001,0.1,20), \"learning_rate_init\": np.linspace(0.001,0.1,20)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(MLPClassifier(), param_grid=param_grid, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = GridSearchCV(MLPClassifier(), param_grid, verbose= 3, cv= 10, n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4800 candidates, totalling 14400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2040 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3864 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4600 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 5400 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6264 tasks      | elapsed: 30.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 36.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8184 tasks      | elapsed: 42.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9240 tasks      | elapsed: 49.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10360 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11544 tasks      | elapsed: 64.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12792 tasks      | elapsed: 72.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14104 tasks      | elapsed: 79.9min\n",
      "[Parallel(n_jobs=-1)]: Done 14400 out of 14400 | elapsed: 81.5min finished\n"
     ]
    }
   ],
   "source": [
    "antes = datetime.now()\n",
    "grid2.fit(X_train, y_train)\n",
    "depois = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-13 23:58:17.600981\n",
      "2018-11-14 01:19:49.153502\n"
     ]
    }
   ],
   "source": [
    "print(antes)\n",
    "print(depois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_learning_rate_init</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977284</td>\n",
       "      <td>0.532655</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.895004</td>\n",
       "      <td>0.838406</td>\n",
       "      <td>0.678028</td>\n",
       "      <td>0.803865</td>\n",
       "      <td>0.091886</td>\n",
       "      <td>2723</td>\n",
       "      <td>0.880391</td>\n",
       "      <td>0.840942</td>\n",
       "      <td>0.671496</td>\n",
       "      <td>0.797610</td>\n",
       "      <td>0.090619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.546374</td>\n",
       "      <td>0.443092</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.749457</td>\n",
       "      <td>0.698551</td>\n",
       "      <td>0.717912</td>\n",
       "      <td>0.721981</td>\n",
       "      <td>0.020984</td>\n",
       "      <td>3175</td>\n",
       "      <td>0.739398</td>\n",
       "      <td>0.700362</td>\n",
       "      <td>0.717856</td>\n",
       "      <td>0.719206</td>\n",
       "      <td>0.015965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.646116</td>\n",
       "      <td>0.443377</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.938450</td>\n",
       "      <td>0.928261</td>\n",
       "      <td>0.918782</td>\n",
       "      <td>0.928502</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>58</td>\n",
       "      <td>0.938383</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.948569</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>0.004636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.585998</td>\n",
       "      <td>0.050891</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.00621053</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.877625</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.864394</td>\n",
       "      <td>0.876329</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>1221</td>\n",
       "      <td>0.868431</td>\n",
       "      <td>0.888768</td>\n",
       "      <td>0.871061</td>\n",
       "      <td>0.876087</td>\n",
       "      <td>0.009031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.640652</td>\n",
       "      <td>0.150899</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.00621053</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.623461</td>\n",
       "      <td>0.692029</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.641063</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>3480</td>\n",
       "      <td>0.622327</td>\n",
       "      <td>0.696739</td>\n",
       "      <td>0.608475</td>\n",
       "      <td>0.642514</td>\n",
       "      <td>0.038758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.883350</td>\n",
       "      <td>0.240125</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.00621053</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.934106</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>0.929659</td>\n",
       "      <td>0.923188</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>111</td>\n",
       "      <td>0.939108</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.951105</td>\n",
       "      <td>0.940216</td>\n",
       "      <td>0.008475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.662237</td>\n",
       "      <td>0.017762</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0114211</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.881970</td>\n",
       "      <td>0.889855</td>\n",
       "      <td>0.870196</td>\n",
       "      <td>0.880676</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>1087</td>\n",
       "      <td>0.872055</td>\n",
       "      <td>0.888768</td>\n",
       "      <td>0.897863</td>\n",
       "      <td>0.886229</td>\n",
       "      <td>0.010688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.490916</td>\n",
       "      <td>0.126148</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0114211</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.608255</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.608412</td>\n",
       "      <td>0.608213</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3735</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.608113</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.901383</td>\n",
       "      <td>0.296636</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0114211</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.923968</td>\n",
       "      <td>0.885507</td>\n",
       "      <td>0.883974</td>\n",
       "      <td>0.897826</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>586</td>\n",
       "      <td>0.933672</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.897501</td>\n",
       "      <td>0.907613</td>\n",
       "      <td>0.018579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.727157</td>\n",
       "      <td>0.049909</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0166316</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.905865</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.853517</td>\n",
       "      <td>0.879952</td>\n",
       "      <td>0.021374</td>\n",
       "      <td>1104</td>\n",
       "      <td>0.888728</td>\n",
       "      <td>0.873913</td>\n",
       "      <td>0.875045</td>\n",
       "      <td>0.879229</td>\n",
       "      <td>0.006733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.500612</td>\n",
       "      <td>0.088086</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0166316</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3759</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.607751</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.892986</td>\n",
       "      <td>0.178476</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0166316</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.933382</td>\n",
       "      <td>0.923188</td>\n",
       "      <td>0.906454</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>135</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.938768</td>\n",
       "      <td>0.924303</td>\n",
       "      <td>0.932852</td>\n",
       "      <td>0.006192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.708838</td>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0218421</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.847936</td>\n",
       "      <td>0.860145</td>\n",
       "      <td>0.873096</td>\n",
       "      <td>0.860386</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>1972</td>\n",
       "      <td>0.837985</td>\n",
       "      <td>0.873551</td>\n",
       "      <td>0.889895</td>\n",
       "      <td>0.867143</td>\n",
       "      <td>0.021671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.400018</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0218421</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3759</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.608837</td>\n",
       "      <td>0.608092</td>\n",
       "      <td>0.000535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.066609</td>\n",
       "      <td>0.518602</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0218421</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.920348</td>\n",
       "      <td>0.915217</td>\n",
       "      <td>0.926033</td>\n",
       "      <td>0.920531</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>143</td>\n",
       "      <td>0.923161</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.951467</td>\n",
       "      <td>0.935021</td>\n",
       "      <td>0.012002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.680257</td>\n",
       "      <td>0.040879</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0270526</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.855177</td>\n",
       "      <td>0.889130</td>\n",
       "      <td>0.858593</td>\n",
       "      <td>0.867633</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>1566</td>\n",
       "      <td>0.848858</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.875770</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>0.015621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.397392</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0270526</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.606807</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>0.606962</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>4112</td>\n",
       "      <td>0.609279</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607751</td>\n",
       "      <td>0.608334</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.295988</td>\n",
       "      <td>0.272115</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0270526</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.934830</td>\n",
       "      <td>0.910870</td>\n",
       "      <td>0.918782</td>\n",
       "      <td>0.921498</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>126</td>\n",
       "      <td>0.932222</td>\n",
       "      <td>0.931522</td>\n",
       "      <td>0.941688</td>\n",
       "      <td>0.935144</td>\n",
       "      <td>0.004636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.459415</td>\n",
       "      <td>0.126721</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0322632</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.824765</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.872371</td>\n",
       "      <td>0.862802</td>\n",
       "      <td>0.027999</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.829286</td>\n",
       "      <td>0.884420</td>\n",
       "      <td>0.884100</td>\n",
       "      <td>0.865935</td>\n",
       "      <td>0.025915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.614808</td>\n",
       "      <td>0.136563</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0322632</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.606807</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>4032</td>\n",
       "      <td>0.608191</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.608475</td>\n",
       "      <td>0.608454</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.182052</td>\n",
       "      <td>0.360184</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0322632</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.939175</td>\n",
       "      <td>0.922464</td>\n",
       "      <td>0.927484</td>\n",
       "      <td>0.929710</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>51</td>\n",
       "      <td>0.945995</td>\n",
       "      <td>0.935145</td>\n",
       "      <td>0.951467</td>\n",
       "      <td>0.944202</td>\n",
       "      <td>0.006783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.641823</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0374737</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.891383</td>\n",
       "      <td>0.871739</td>\n",
       "      <td>0.868745</td>\n",
       "      <td>0.877295</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>1191</td>\n",
       "      <td>0.888003</td>\n",
       "      <td>0.872101</td>\n",
       "      <td>0.889171</td>\n",
       "      <td>0.883092</td>\n",
       "      <td>0.007786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.490738</td>\n",
       "      <td>0.137022</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0374737</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607488</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3923</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.608113</td>\n",
       "      <td>0.608213</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.342576</td>\n",
       "      <td>0.379559</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0374737</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.927589</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.923133</td>\n",
       "      <td>0.927053</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>72</td>\n",
       "      <td>0.928597</td>\n",
       "      <td>0.940217</td>\n",
       "      <td>0.952916</td>\n",
       "      <td>0.940577</td>\n",
       "      <td>0.009931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.394155</td>\n",
       "      <td>0.235910</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0426842</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.887038</td>\n",
       "      <td>0.831884</td>\n",
       "      <td>0.852792</td>\n",
       "      <td>0.857246</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>2114</td>\n",
       "      <td>0.877129</td>\n",
       "      <td>0.848913</td>\n",
       "      <td>0.873234</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>0.012485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.443801</td>\n",
       "      <td>0.162818</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0426842</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.608412</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>3741</td>\n",
       "      <td>0.608191</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.608475</td>\n",
       "      <td>0.608454</td>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.954204</td>\n",
       "      <td>0.228504</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0426842</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.933382</td>\n",
       "      <td>0.924638</td>\n",
       "      <td>0.903553</td>\n",
       "      <td>0.920531</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>143</td>\n",
       "      <td>0.926423</td>\n",
       "      <td>0.937681</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.795061</td>\n",
       "      <td>0.063376</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0478947</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.868211</td>\n",
       "      <td>0.759420</td>\n",
       "      <td>0.847716</td>\n",
       "      <td>0.825121</td>\n",
       "      <td>0.047205</td>\n",
       "      <td>2583</td>\n",
       "      <td>0.849583</td>\n",
       "      <td>0.767754</td>\n",
       "      <td>0.853314</td>\n",
       "      <td>0.823550</td>\n",
       "      <td>0.039484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.471159</td>\n",
       "      <td>0.048541</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0478947</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607488</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3923</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.608837</td>\n",
       "      <td>0.608212</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.160433</td>\n",
       "      <td>0.487499</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>relu</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.0478947</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 1e-05, 'learni...</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.913768</td>\n",
       "      <td>0.923133</td>\n",
       "      <td>0.924396</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>97</td>\n",
       "      <td>0.929685</td>\n",
       "      <td>0.935507</td>\n",
       "      <td>0.940601</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.004460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>2.374420</td>\n",
       "      <td>0.056385</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0531053</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.873280</td>\n",
       "      <td>0.868116</td>\n",
       "      <td>0.857868</td>\n",
       "      <td>0.866425</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>1629</td>\n",
       "      <td>0.868068</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.880840</td>\n",
       "      <td>0.869201</td>\n",
       "      <td>0.009076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>0.354815</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0531053</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.649529</td>\n",
       "      <td>0.627536</td>\n",
       "      <td>0.722988</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.040804</td>\n",
       "      <td>3357</td>\n",
       "      <td>0.644436</td>\n",
       "      <td>0.623913</td>\n",
       "      <td>0.697936</td>\n",
       "      <td>0.655428</td>\n",
       "      <td>0.031203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>1.091751</td>\n",
       "      <td>0.319360</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0531053</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.914555</td>\n",
       "      <td>0.831884</td>\n",
       "      <td>0.787527</td>\n",
       "      <td>0.844686</td>\n",
       "      <td>0.052643</td>\n",
       "      <td>2413</td>\n",
       "      <td>0.914099</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>0.806230</td>\n",
       "      <td>0.850375</td>\n",
       "      <td>0.046162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4773</th>\n",
       "      <td>2.238263</td>\n",
       "      <td>0.019991</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0583158</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.889211</td>\n",
       "      <td>0.860870</td>\n",
       "      <td>0.865120</td>\n",
       "      <td>0.871739</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>1370</td>\n",
       "      <td>0.885466</td>\n",
       "      <td>0.863043</td>\n",
       "      <td>0.879754</td>\n",
       "      <td>0.876088</td>\n",
       "      <td>0.009514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4774</th>\n",
       "      <td>0.434167</td>\n",
       "      <td>0.060186</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0583158</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.548154</td>\n",
       "      <td>0.605797</td>\n",
       "      <td>0.612763</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.028960</td>\n",
       "      <td>4208</td>\n",
       "      <td>0.554549</td>\n",
       "      <td>0.610145</td>\n",
       "      <td>0.620790</td>\n",
       "      <td>0.595161</td>\n",
       "      <td>0.029044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4775</th>\n",
       "      <td>1.256977</td>\n",
       "      <td>0.297577</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0583158</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.900072</td>\n",
       "      <td>0.893478</td>\n",
       "      <td>0.765772</td>\n",
       "      <td>0.853140</td>\n",
       "      <td>0.061803</td>\n",
       "      <td>2256</td>\n",
       "      <td>0.897789</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>0.772546</td>\n",
       "      <td>0.858711</td>\n",
       "      <td>0.061015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>2.215814</td>\n",
       "      <td>0.015939</td>\n",
       "      <td>0.004749</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0635263</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.866039</td>\n",
       "      <td>0.881884</td>\n",
       "      <td>0.860769</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>1465</td>\n",
       "      <td>0.868068</td>\n",
       "      <td>0.876449</td>\n",
       "      <td>0.880116</td>\n",
       "      <td>0.874878</td>\n",
       "      <td>0.005042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>0.374449</td>\n",
       "      <td>0.021289</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0635263</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.642288</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.632342</td>\n",
       "      <td>0.649517</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>3436</td>\n",
       "      <td>0.643711</td>\n",
       "      <td>0.668841</td>\n",
       "      <td>0.621876</td>\n",
       "      <td>0.644809</td>\n",
       "      <td>0.019189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>0.806650</td>\n",
       "      <td>0.198896</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0635263</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.876901</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.816534</td>\n",
       "      <td>0.851932</td>\n",
       "      <td>0.025716</td>\n",
       "      <td>2284</td>\n",
       "      <td>0.880029</td>\n",
       "      <td>0.867391</td>\n",
       "      <td>0.819631</td>\n",
       "      <td>0.855684</td>\n",
       "      <td>0.026010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779</th>\n",
       "      <td>2.326243</td>\n",
       "      <td>0.012521</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0687368</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.880521</td>\n",
       "      <td>0.863043</td>\n",
       "      <td>0.875997</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>1312</td>\n",
       "      <td>0.879304</td>\n",
       "      <td>0.853986</td>\n",
       "      <td>0.890257</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.015190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>0.384857</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0687368</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.590876</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.602174</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>4174</td>\n",
       "      <td>0.598405</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.607751</td>\n",
       "      <td>0.604588</td>\n",
       "      <td>0.004372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>0.710566</td>\n",
       "      <td>0.101149</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0687368</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.814627</td>\n",
       "      <td>0.763768</td>\n",
       "      <td>0.824511</td>\n",
       "      <td>0.800966</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>2736</td>\n",
       "      <td>0.814426</td>\n",
       "      <td>0.769203</td>\n",
       "      <td>0.833032</td>\n",
       "      <td>0.805553</td>\n",
       "      <td>0.026802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>2.350608</td>\n",
       "      <td>0.022182</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0739474</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.858798</td>\n",
       "      <td>0.868841</td>\n",
       "      <td>0.839014</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>2179</td>\n",
       "      <td>0.860094</td>\n",
       "      <td>0.860507</td>\n",
       "      <td>0.858385</td>\n",
       "      <td>0.859662</td>\n",
       "      <td>0.000919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>0.367164</td>\n",
       "      <td>0.025852</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0739474</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.584359</td>\n",
       "      <td>0.642029</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.632126</td>\n",
       "      <td>0.035677</td>\n",
       "      <td>3543</td>\n",
       "      <td>0.594418</td>\n",
       "      <td>0.640580</td>\n",
       "      <td>0.666425</td>\n",
       "      <td>0.633808</td>\n",
       "      <td>0.029784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>0.592090</td>\n",
       "      <td>0.107632</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0739474</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.669804</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>0.608412</td>\n",
       "      <td>0.628502</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>3571</td>\n",
       "      <td>0.666546</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.611373</td>\n",
       "      <td>0.628751</td>\n",
       "      <td>0.026754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>2.423291</td>\n",
       "      <td>0.068201</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0791579</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.864591</td>\n",
       "      <td>0.860870</td>\n",
       "      <td>0.866570</td>\n",
       "      <td>0.864010</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>1752</td>\n",
       "      <td>0.864806</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.884462</td>\n",
       "      <td>0.866906</td>\n",
       "      <td>0.013559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>0.347788</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0791579</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3759</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.607751</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>0.661475</td>\n",
       "      <td>0.104017</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0791579</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.826937</td>\n",
       "      <td>0.762319</td>\n",
       "      <td>0.856418</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.039294</td>\n",
       "      <td>2646</td>\n",
       "      <td>0.823849</td>\n",
       "      <td>0.756884</td>\n",
       "      <td>0.861644</td>\n",
       "      <td>0.814126</td>\n",
       "      <td>0.043317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>2.430430</td>\n",
       "      <td>0.015967</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0843684</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.886314</td>\n",
       "      <td>0.857971</td>\n",
       "      <td>0.826686</td>\n",
       "      <td>0.857005</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>2127</td>\n",
       "      <td>0.884016</td>\n",
       "      <td>0.857246</td>\n",
       "      <td>0.849330</td>\n",
       "      <td>0.863531</td>\n",
       "      <td>0.014841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>0.383547</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0843684</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.611151</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.608937</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>3727</td>\n",
       "      <td>0.639725</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.607751</td>\n",
       "      <td>0.618361</td>\n",
       "      <td>0.015106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4790</th>\n",
       "      <td>0.664532</td>\n",
       "      <td>0.104585</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0843684</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.884142</td>\n",
       "      <td>0.830435</td>\n",
       "      <td>0.611313</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>0.117993</td>\n",
       "      <td>2894</td>\n",
       "      <td>0.878217</td>\n",
       "      <td>0.847464</td>\n",
       "      <td>0.617168</td>\n",
       "      <td>0.780949</td>\n",
       "      <td>0.116490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>2.363827</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0895789</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.874004</td>\n",
       "      <td>0.882609</td>\n",
       "      <td>0.851342</td>\n",
       "      <td>0.869324</td>\n",
       "      <td>0.013185</td>\n",
       "      <td>1482</td>\n",
       "      <td>0.866256</td>\n",
       "      <td>0.870652</td>\n",
       "      <td>0.871423</td>\n",
       "      <td>0.869444</td>\n",
       "      <td>0.002276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4792</th>\n",
       "      <td>0.368562</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0895789</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>3759</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.607751</td>\n",
       "      <td>0.607729</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4793</th>\n",
       "      <td>0.968974</td>\n",
       "      <td>0.201561</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0895789</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.858074</td>\n",
       "      <td>0.856522</td>\n",
       "      <td>0.626541</td>\n",
       "      <td>0.780435</td>\n",
       "      <td>0.108762</td>\n",
       "      <td>2866</td>\n",
       "      <td>0.860819</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.632380</td>\n",
       "      <td>0.785173</td>\n",
       "      <td>0.108043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4794</th>\n",
       "      <td>2.361063</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0947895</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.882694</td>\n",
       "      <td>0.868116</td>\n",
       "      <td>0.865845</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>1354</td>\n",
       "      <td>0.872780</td>\n",
       "      <td>0.867391</td>\n",
       "      <td>0.888446</td>\n",
       "      <td>0.876206</td>\n",
       "      <td>0.008930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>0.364479</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0947895</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.607531</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.612038</td>\n",
       "      <td>0.609179</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>3725</td>\n",
       "      <td>0.607829</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.617892</td>\n",
       "      <td>0.611110</td>\n",
       "      <td>0.004797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>0.775341</td>\n",
       "      <td>0.346447</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0947895</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.892107</td>\n",
       "      <td>0.620290</td>\n",
       "      <td>0.810007</td>\n",
       "      <td>0.774155</td>\n",
       "      <td>0.113845</td>\n",
       "      <td>2901</td>\n",
       "      <td>0.895252</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.816009</td>\n",
       "      <td>0.775734</td>\n",
       "      <td>0.117530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>2.457084</td>\n",
       "      <td>0.075286</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.857350</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.854967</td>\n",
       "      <td>0.860628</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>1960</td>\n",
       "      <td>0.853570</td>\n",
       "      <td>0.867391</td>\n",
       "      <td>0.868164</td>\n",
       "      <td>0.863042</td>\n",
       "      <td>0.006705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>0.369176</td>\n",
       "      <td>0.016226</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.449674</td>\n",
       "      <td>0.607971</td>\n",
       "      <td>0.637418</td>\n",
       "      <td>0.564976</td>\n",
       "      <td>0.082456</td>\n",
       "      <td>4266</td>\n",
       "      <td>0.462849</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.637450</td>\n",
       "      <td>0.569303</td>\n",
       "      <td>0.076254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0.425282</td>\n",
       "      <td>0.099951</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'alpha': 0.1, 'lear...</td>\n",
       "      <td>0.832730</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.859318</td>\n",
       "      <td>0.826329</td>\n",
       "      <td>0.029881</td>\n",
       "      <td>2576</td>\n",
       "      <td>0.837260</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.877943</td>\n",
       "      <td>0.832604</td>\n",
       "      <td>0.039059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0          0.977284      0.532655         0.003745        0.000352   \n",
       "1          1.546374      0.443092         0.002787        0.000293   \n",
       "2          1.646116      0.443377         0.002673        0.000153   \n",
       "3          1.585998      0.050891         0.002870        0.000432   \n",
       "4          0.640652      0.150899         0.002548        0.000299   \n",
       "5          0.883350      0.240125         0.002885        0.000322   \n",
       "6          1.662237      0.017762         0.003288        0.000577   \n",
       "7          0.490916      0.126148         0.002900        0.000450   \n",
       "8          0.901383      0.296636         0.002955        0.000274   \n",
       "9          1.727157      0.049909         0.002886        0.000336   \n",
       "10         0.500612      0.088086         0.002891        0.000476   \n",
       "11         0.892986      0.178476         0.003870        0.001202   \n",
       "12         1.708838      0.034250         0.002851        0.000301   \n",
       "13         0.400018      0.004175         0.002783        0.000142   \n",
       "14         1.066609      0.518602         0.003063        0.000223   \n",
       "15         1.680257      0.040879         0.002570        0.000080   \n",
       "16         0.397392      0.070351         0.002578        0.000453   \n",
       "17         1.295988      0.272115         0.003028        0.000128   \n",
       "18         1.459415      0.126721         0.002976        0.000463   \n",
       "19         0.614808      0.136563         0.002495        0.000218   \n",
       "20         1.182052      0.360184         0.002928        0.000378   \n",
       "21         1.641823      0.013810         0.003158        0.000231   \n",
       "22         0.490738      0.137022         0.003362        0.000950   \n",
       "23         1.342576      0.379559         0.002716        0.000120   \n",
       "24         1.394155      0.235910         0.002456        0.000264   \n",
       "25         0.443801      0.162818         0.002615        0.000085   \n",
       "26         0.954204      0.228504         0.002933        0.000226   \n",
       "27         1.795061      0.063376         0.002597        0.000370   \n",
       "28         0.471159      0.048541         0.002523        0.000266   \n",
       "29         1.160433      0.487499         0.003600        0.000305   \n",
       "...             ...           ...              ...             ...   \n",
       "4770       2.374420      0.056385         0.004402        0.000251   \n",
       "4771       0.354815      0.016897         0.004554        0.000678   \n",
       "4772       1.091751      0.319360         0.004605        0.000502   \n",
       "4773       2.238263      0.019991         0.004660        0.000113   \n",
       "4774       0.434167      0.060186         0.004497        0.000386   \n",
       "4775       1.256977      0.297577         0.005105        0.000412   \n",
       "4776       2.215814      0.015939         0.004749        0.000130   \n",
       "4777       0.374449      0.021289         0.004441        0.000360   \n",
       "4778       0.806650      0.198896         0.004467        0.000408   \n",
       "4779       2.326243      0.012521         0.004312        0.000143   \n",
       "4780       0.384857      0.010768         0.004509        0.000500   \n",
       "4781       0.710566      0.101149         0.005231        0.000542   \n",
       "4782       2.350608      0.022182         0.005051        0.000639   \n",
       "4783       0.367164      0.025852         0.005262        0.000490   \n",
       "4784       0.592090      0.107632         0.004212        0.000067   \n",
       "4785       2.423291      0.068201         0.004541        0.000306   \n",
       "4786       0.347788      0.003146         0.004427        0.000543   \n",
       "4787       0.661475      0.104017         0.004984        0.000639   \n",
       "4788       2.430430      0.015967         0.005462        0.001225   \n",
       "4789       0.383547      0.008406         0.004234        0.000530   \n",
       "4790       0.664532      0.104585         0.004944        0.000635   \n",
       "4791       2.363827      0.013442         0.004783        0.000600   \n",
       "4792       0.368562      0.026674         0.004724        0.000459   \n",
       "4793       0.968974      0.201561         0.005051        0.001597   \n",
       "4794       2.361063      0.110200         0.005385        0.000624   \n",
       "4795       0.364479      0.041719         0.005776        0.000751   \n",
       "4796       0.775341      0.346447         0.004494        0.000208   \n",
       "4797       2.457084      0.075286         0.004196        0.000299   \n",
       "4798       0.369176      0.016226         0.004306        0.000531   \n",
       "4799       0.425282      0.099951         0.003723        0.000240   \n",
       "\n",
       "     param_activation param_alpha param_learning_rate_init param_solver  \\\n",
       "0                relu       1e-05                    0.001        lbfgs   \n",
       "1                relu       1e-05                    0.001          sgd   \n",
       "2                relu       1e-05                    0.001         adam   \n",
       "3                relu       1e-05               0.00621053        lbfgs   \n",
       "4                relu       1e-05               0.00621053          sgd   \n",
       "5                relu       1e-05               0.00621053         adam   \n",
       "6                relu       1e-05                0.0114211        lbfgs   \n",
       "7                relu       1e-05                0.0114211          sgd   \n",
       "8                relu       1e-05                0.0114211         adam   \n",
       "9                relu       1e-05                0.0166316        lbfgs   \n",
       "10               relu       1e-05                0.0166316          sgd   \n",
       "11               relu       1e-05                0.0166316         adam   \n",
       "12               relu       1e-05                0.0218421        lbfgs   \n",
       "13               relu       1e-05                0.0218421          sgd   \n",
       "14               relu       1e-05                0.0218421         adam   \n",
       "15               relu       1e-05                0.0270526        lbfgs   \n",
       "16               relu       1e-05                0.0270526          sgd   \n",
       "17               relu       1e-05                0.0270526         adam   \n",
       "18               relu       1e-05                0.0322632        lbfgs   \n",
       "19               relu       1e-05                0.0322632          sgd   \n",
       "20               relu       1e-05                0.0322632         adam   \n",
       "21               relu       1e-05                0.0374737        lbfgs   \n",
       "22               relu       1e-05                0.0374737          sgd   \n",
       "23               relu       1e-05                0.0374737         adam   \n",
       "24               relu       1e-05                0.0426842        lbfgs   \n",
       "25               relu       1e-05                0.0426842          sgd   \n",
       "26               relu       1e-05                0.0426842         adam   \n",
       "27               relu       1e-05                0.0478947        lbfgs   \n",
       "28               relu       1e-05                0.0478947          sgd   \n",
       "29               relu       1e-05                0.0478947         adam   \n",
       "...               ...         ...                      ...          ...   \n",
       "4770         logistic         0.1                0.0531053        lbfgs   \n",
       "4771         logistic         0.1                0.0531053          sgd   \n",
       "4772         logistic         0.1                0.0531053         adam   \n",
       "4773         logistic         0.1                0.0583158        lbfgs   \n",
       "4774         logistic         0.1                0.0583158          sgd   \n",
       "4775         logistic         0.1                0.0583158         adam   \n",
       "4776         logistic         0.1                0.0635263        lbfgs   \n",
       "4777         logistic         0.1                0.0635263          sgd   \n",
       "4778         logistic         0.1                0.0635263         adam   \n",
       "4779         logistic         0.1                0.0687368        lbfgs   \n",
       "4780         logistic         0.1                0.0687368          sgd   \n",
       "4781         logistic         0.1                0.0687368         adam   \n",
       "4782         logistic         0.1                0.0739474        lbfgs   \n",
       "4783         logistic         0.1                0.0739474          sgd   \n",
       "4784         logistic         0.1                0.0739474         adam   \n",
       "4785         logistic         0.1                0.0791579        lbfgs   \n",
       "4786         logistic         0.1                0.0791579          sgd   \n",
       "4787         logistic         0.1                0.0791579         adam   \n",
       "4788         logistic         0.1                0.0843684        lbfgs   \n",
       "4789         logistic         0.1                0.0843684          sgd   \n",
       "4790         logistic         0.1                0.0843684         adam   \n",
       "4791         logistic         0.1                0.0895789        lbfgs   \n",
       "4792         logistic         0.1                0.0895789          sgd   \n",
       "4793         logistic         0.1                0.0895789         adam   \n",
       "4794         logistic         0.1                0.0947895        lbfgs   \n",
       "4795         logistic         0.1                0.0947895          sgd   \n",
       "4796         logistic         0.1                0.0947895         adam   \n",
       "4797         logistic         0.1                      0.1        lbfgs   \n",
       "4798         logistic         0.1                      0.1          sgd   \n",
       "4799         logistic         0.1                      0.1         adam   \n",
       "\n",
       "                                                 params  split0_test_score  \\\n",
       "0     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.895004   \n",
       "1     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.749457   \n",
       "2     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.938450   \n",
       "3     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.877625   \n",
       "4     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.623461   \n",
       "5     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.934106   \n",
       "6     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.881970   \n",
       "7     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.608255   \n",
       "8     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.923968   \n",
       "9     {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.905865   \n",
       "10    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.607531   \n",
       "11    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.933382   \n",
       "12    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.847936   \n",
       "13    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.607531   \n",
       "14    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.920348   \n",
       "15    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.855177   \n",
       "16    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.606807   \n",
       "17    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.934830   \n",
       "18    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.824765   \n",
       "19    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.606807   \n",
       "20    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.939175   \n",
       "21    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.891383   \n",
       "22    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.607531   \n",
       "23    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.927589   \n",
       "24    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.887038   \n",
       "25    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.607531   \n",
       "26    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.933382   \n",
       "27    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.868211   \n",
       "28    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.607531   \n",
       "29    {'activation': 'relu', 'alpha': 1e-05, 'learni...           0.936278   \n",
       "...                                                 ...                ...   \n",
       "4770  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.873280   \n",
       "4771  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.649529   \n",
       "4772  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.914555   \n",
       "4773  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.889211   \n",
       "4774  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.548154   \n",
       "4775  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.900072   \n",
       "4776  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.866039   \n",
       "4777  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.642288   \n",
       "4778  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.876901   \n",
       "4779  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.880521   \n",
       "4780  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.590876   \n",
       "4781  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.814627   \n",
       "4782  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.858798   \n",
       "4783  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.584359   \n",
       "4784  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.669804   \n",
       "4785  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.864591   \n",
       "4786  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.607531   \n",
       "4787  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.826937   \n",
       "4788  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.886314   \n",
       "4789  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.611151   \n",
       "4790  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.884142   \n",
       "4791  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.874004   \n",
       "4792  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.607531   \n",
       "4793  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.858074   \n",
       "4794  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.882694   \n",
       "4795  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.607531   \n",
       "4796  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.892107   \n",
       "4797  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.857350   \n",
       "4798  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.449674   \n",
       "4799  {'activation': 'logistic', 'alpha': 0.1, 'lear...           0.832730   \n",
       "\n",
       "      split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0              0.838406           0.678028         0.803865        0.091886   \n",
       "1              0.698551           0.717912         0.721981        0.020984   \n",
       "2              0.928261           0.918782         0.928502        0.008032   \n",
       "3              0.886957           0.864394         0.876329        0.009255   \n",
       "4              0.692029           0.607687         0.641063        0.036609   \n",
       "5              0.905797           0.929659         0.923188        0.012431   \n",
       "6              0.889855           0.870196         0.880676        0.008076   \n",
       "7              0.607971           0.608412         0.608213        0.000182   \n",
       "8              0.885507           0.883974         0.897826        0.018506   \n",
       "9              0.880435           0.853517         0.879952        0.021374   \n",
       "10             0.607971           0.607687         0.607729        0.000182   \n",
       "11             0.923188           0.906454         0.921014        0.011100   \n",
       "12             0.860145           0.873096         0.860386        0.010273   \n",
       "13             0.607971           0.607687         0.607729        0.000182   \n",
       "14             0.915217           0.926033         0.920531        0.004417   \n",
       "15             0.889130           0.858593         0.867633        0.015265   \n",
       "16             0.607246           0.606962         0.607005        0.000182   \n",
       "17             0.910870           0.918782         0.921498        0.009970   \n",
       "18             0.891304           0.872371         0.862802        0.027999   \n",
       "19             0.607246           0.607687         0.607246        0.000359   \n",
       "20             0.922464           0.927484         0.929710        0.007002   \n",
       "21             0.871739           0.868745         0.877295        0.010042   \n",
       "22             0.607246           0.607687         0.607488        0.000182   \n",
       "23             0.930435           0.923133         0.927053        0.003005   \n",
       "24             0.831884           0.852792         0.857246        0.022740   \n",
       "25             0.607971           0.608412         0.607971        0.000360   \n",
       "26             0.924638           0.903553         0.920531        0.012519   \n",
       "27             0.759420           0.847716         0.825121        0.047205   \n",
       "28             0.607246           0.607687         0.607488        0.000182   \n",
       "29             0.913768           0.923133         0.924396        0.009235   \n",
       "...                 ...                ...              ...             ...   \n",
       "4770           0.868116           0.857868         0.866425        0.006405   \n",
       "4771           0.627536           0.722988         0.666667        0.040804   \n",
       "4772           0.831884           0.787527         0.844686        0.052643   \n",
       "4773           0.860870           0.865120         0.871739        0.012482   \n",
       "4774           0.605797           0.612763         0.588889        0.028960   \n",
       "4775           0.893478           0.765772         0.853140        0.061803   \n",
       "4776           0.881884           0.860769         0.869565        0.008973   \n",
       "4777           0.673913           0.632342         0.649517        0.017722   \n",
       "4778           0.862319           0.816534         0.851932        0.025716   \n",
       "4779           0.863043           0.875997         0.873188        0.007408   \n",
       "4780           0.607971           0.607687         0.602174        0.007994   \n",
       "4781           0.763768           0.824511         0.800966        0.026611   \n",
       "4782           0.868841           0.839014         0.855556        0.012389   \n",
       "4783           0.642029           0.670051         0.632126        0.035677   \n",
       "4784           0.607246           0.608412         0.628502        0.029225   \n",
       "4785           0.860870           0.866570         0.864010        0.002363   \n",
       "4786           0.607971           0.607687         0.607729        0.000182   \n",
       "4787           0.762319           0.856418         0.815217        0.039294   \n",
       "4788           0.857971           0.826686         0.857005        0.024353   \n",
       "4789           0.607971           0.607687         0.608937        0.001571   \n",
       "4790           0.830435           0.611313         0.775362        0.117993   \n",
       "4791           0.882609           0.851342         0.869324        0.013185   \n",
       "4792           0.607971           0.607687         0.607729        0.000182   \n",
       "4793           0.856522           0.626541         0.780435        0.108762   \n",
       "4794           0.868116           0.865845         0.872222        0.007466   \n",
       "4795           0.607971           0.612038         0.609179        0.002028   \n",
       "4796           0.620290           0.810007         0.774155        0.113845   \n",
       "4797           0.869565           0.854967         0.860628        0.006394   \n",
       "4798           0.607971           0.637418         0.564976        0.082456   \n",
       "4799           0.786957           0.859318         0.826329        0.029881   \n",
       "\n",
       "      rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                2723            0.880391            0.840942   \n",
       "1                3175            0.739398            0.700362   \n",
       "2                  58            0.938383            0.947826   \n",
       "3                1221            0.868431            0.888768   \n",
       "4                3480            0.622327            0.696739   \n",
       "5                 111            0.939108            0.930435   \n",
       "6                1087            0.872055            0.888768   \n",
       "7                3735            0.607829            0.607971   \n",
       "8                 586            0.933672            0.891667   \n",
       "9                1104            0.888728            0.873913   \n",
       "10               3759            0.607829            0.608333   \n",
       "11                135            0.935484            0.938768   \n",
       "12               1972            0.837985            0.873551   \n",
       "13               3759            0.607829            0.607609   \n",
       "14                143            0.923161            0.930435   \n",
       "15               1566            0.848858            0.885870   \n",
       "16               4112            0.609279            0.607971   \n",
       "17                126            0.932222            0.931522   \n",
       "18               1836            0.829286            0.884420   \n",
       "19               4032            0.608191            0.608696   \n",
       "20                 51            0.945995            0.935145   \n",
       "21               1191            0.888003            0.872101   \n",
       "22               3923            0.607829            0.608696   \n",
       "23                 72            0.928597            0.940217   \n",
       "24               2114            0.877129            0.848913   \n",
       "25               3741            0.608191            0.608696   \n",
       "26                143            0.926423            0.937681   \n",
       "27               2583            0.849583            0.767754   \n",
       "28               3923            0.607829            0.607971   \n",
       "29                 97            0.929685            0.935507   \n",
       "...               ...                 ...                 ...   \n",
       "4770             1629            0.868068            0.858696   \n",
       "4771             3357            0.644436            0.623913   \n",
       "4772             2413            0.914099            0.830797   \n",
       "4773             1370            0.885466            0.863043   \n",
       "4774             4208            0.554549            0.610145   \n",
       "4775             2256            0.897789            0.905797   \n",
       "4776             1465            0.868068            0.876449   \n",
       "4777             3436            0.643711            0.668841   \n",
       "4778             2284            0.880029            0.867391   \n",
       "4779             1312            0.879304            0.853986   \n",
       "4780             4174            0.598405            0.607609   \n",
       "4781             2736            0.814426            0.769203   \n",
       "4782             2179            0.860094            0.860507   \n",
       "4783             3543            0.594418            0.640580   \n",
       "4784             3571            0.666546            0.608333   \n",
       "4785             1752            0.864806            0.851449   \n",
       "4786             3759            0.607829            0.607609   \n",
       "4787             2646            0.823849            0.756884   \n",
       "4788             2127            0.884016            0.857246   \n",
       "4789             3727            0.639725            0.607609   \n",
       "4790             2894            0.878217            0.847464   \n",
       "4791             1482            0.866256            0.870652   \n",
       "4792             3759            0.607829            0.607609   \n",
       "4793             2866            0.860819            0.862319   \n",
       "4794             1354            0.872780            0.867391   \n",
       "4795             3725            0.607829            0.607609   \n",
       "4796             2901            0.895252            0.615942   \n",
       "4797             1960            0.853570            0.867391   \n",
       "4798             4266            0.462849            0.607609   \n",
       "4799             2576            0.837260            0.782609   \n",
       "\n",
       "      split2_train_score  mean_train_score  std_train_score  \n",
       "0               0.671496          0.797610         0.090619  \n",
       "1               0.717856          0.719206         0.015965  \n",
       "2               0.948569          0.944926         0.004636  \n",
       "3               0.871061          0.876087         0.009031  \n",
       "4               0.608475          0.642514         0.038758  \n",
       "5               0.951105          0.940216         0.008475  \n",
       "6               0.897863          0.886229         0.010688  \n",
       "7               0.608113          0.607971         0.000116  \n",
       "8               0.897501          0.907613         0.018579  \n",
       "9               0.875045          0.879229         0.006733  \n",
       "10              0.607751          0.607971         0.000258  \n",
       "11              0.924303          0.932852         0.006192  \n",
       "12              0.889895          0.867143         0.021671  \n",
       "13              0.608837          0.608092         0.000535  \n",
       "14              0.951467          0.935021         0.012002  \n",
       "15              0.875770          0.870166         0.015621  \n",
       "16              0.607751          0.608334         0.000674  \n",
       "17              0.941688          0.935144         0.004636  \n",
       "18              0.884100          0.865935         0.025915  \n",
       "19              0.608475          0.608454         0.000206  \n",
       "20              0.951467          0.944202         0.006783  \n",
       "21              0.889171          0.883092         0.007786  \n",
       "22              0.608113          0.608213         0.000361  \n",
       "23              0.952916          0.940577         0.009931  \n",
       "24              0.873234          0.866426         0.012485  \n",
       "25              0.608475          0.608454         0.000206  \n",
       "26              0.927200          0.930435         0.005134  \n",
       "27              0.853314          0.823550         0.039484  \n",
       "28              0.608837          0.608212         0.000446  \n",
       "29              0.940601          0.935264         0.004460  \n",
       "...                  ...               ...              ...  \n",
       "4770            0.880840          0.869201         0.009076  \n",
       "4771            0.697936          0.655428         0.031203  \n",
       "4772            0.806230          0.850375         0.046162  \n",
       "4773            0.879754          0.876088         0.009514  \n",
       "4774            0.620790          0.595161         0.029044  \n",
       "4775            0.772546          0.858711         0.061015  \n",
       "4776            0.880116          0.874878         0.005042  \n",
       "4777            0.621876          0.644809         0.019189  \n",
       "4778            0.819631          0.855684         0.026010  \n",
       "4779            0.890257          0.874516         0.015190  \n",
       "4780            0.607751          0.604588         0.004372  \n",
       "4781            0.833032          0.805553         0.026802  \n",
       "4782            0.858385          0.859662         0.000919  \n",
       "4783            0.666425          0.633808         0.029784  \n",
       "4784            0.611373          0.628751         0.026754  \n",
       "4785            0.884462          0.866906         0.013559  \n",
       "4786            0.607751          0.607729         0.000091  \n",
       "4787            0.861644          0.814126         0.043317  \n",
       "4788            0.849330          0.863531         0.014841  \n",
       "4789            0.607751          0.618361         0.015106  \n",
       "4790            0.617168          0.780949         0.116490  \n",
       "4791            0.871423          0.869444         0.002276  \n",
       "4792            0.607751          0.607729         0.000091  \n",
       "4793            0.632380          0.785173         0.108043  \n",
       "4794            0.888446          0.876206         0.008930  \n",
       "4795            0.617892          0.611110         0.004797  \n",
       "4796            0.816009          0.775734         0.117530  \n",
       "4797            0.868164          0.863042         0.006705  \n",
       "4798            0.637450          0.569303         0.076254  \n",
       "4799            0.877943          0.832604         0.039059  \n",
       "\n",
       "[4800 rows x 20 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9410628019323671"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'alpha': 0.015797894736842105,\n",
       " 'learning_rate_init': 0.001,\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grid2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.93      0.94       189\n",
      "           1       0.95      0.97      0.96       272\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       461\n",
      "   macro avg       0.96      0.95      0.95       461\n",
      "weighted avg       0.95      0.95      0.95       461\n",
      "\n",
      "[[175  14]\n",
      " [  7 265]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, result))\n",
    "print(confusion_matrix(y_test, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_learning_rate_init</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.777046</td>\n",
       "      <td>0.180831</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>relu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation': 'relu'}</td>\n",
       "      <td>0.938450</td>\n",
       "      <td>0.915217</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.914493</td>\n",
       "      <td>0.019878</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.794834</td>\n",
       "      <td>0.610527</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>tanh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation': 'tanh'}</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.929710</td>\n",
       "      <td>0.947063</td>\n",
       "      <td>0.939130</td>\n",
       "      <td>0.007161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.385262</td>\n",
       "      <td>0.040113</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>identity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation': 'identity'}</td>\n",
       "      <td>0.902969</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.865845</td>\n",
       "      <td>0.896860</td>\n",
       "      <td>0.023220</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.016187</td>\n",
       "      <td>0.142113</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>logistic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'activation': 'logistic'}</td>\n",
       "      <td>0.939175</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.941262</td>\n",
       "      <td>0.936957</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857794</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'solver': 'lbfgs'}</td>\n",
       "      <td>0.898624</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.871646</td>\n",
       "      <td>0.889614</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.690793</td>\n",
       "      <td>0.129418</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sgd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'solver': 'sgd'}</td>\n",
       "      <td>0.764663</td>\n",
       "      <td>0.686957</td>\n",
       "      <td>0.754895</td>\n",
       "      <td>0.735507</td>\n",
       "      <td>0.034561</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.563491</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'solver': 'adam'}</td>\n",
       "      <td>0.893555</td>\n",
       "      <td>0.910870</td>\n",
       "      <td>0.925308</td>\n",
       "      <td>0.909903</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.779568</td>\n",
       "      <td>0.267565</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 1e-05}</td>\n",
       "      <td>0.912382</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.914431</td>\n",
       "      <td>0.914493</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.632248</td>\n",
       "      <td>0.221637</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00527263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.005272631578947368}</td>\n",
       "      <td>0.921796</td>\n",
       "      <td>0.925362</td>\n",
       "      <td>0.918057</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.782797</td>\n",
       "      <td>0.208850</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0105353</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.010535263157894737}</td>\n",
       "      <td>0.945692</td>\n",
       "      <td>0.929710</td>\n",
       "      <td>0.920232</td>\n",
       "      <td>0.931884</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.863215</td>\n",
       "      <td>0.300807</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0157979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.015797894736842105}</td>\n",
       "      <td>0.935554</td>\n",
       "      <td>0.889855</td>\n",
       "      <td>0.922408</td>\n",
       "      <td>0.915942</td>\n",
       "      <td>0.019211</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.961452</td>\n",
       "      <td>0.175223</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0210605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.021060526315789474}</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.924638</td>\n",
       "      <td>0.899202</td>\n",
       "      <td>0.921498</td>\n",
       "      <td>0.017055</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.780386</td>\n",
       "      <td>0.100554</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0263232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.026323157894736843}</td>\n",
       "      <td>0.939899</td>\n",
       "      <td>0.910145</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>0.920773</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.584565</td>\n",
       "      <td>0.096684</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0315858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.03158578947368421}</td>\n",
       "      <td>0.932657</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.931835</td>\n",
       "      <td>0.931643</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.581026</td>\n",
       "      <td>0.111069</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0368484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.036848421052631586}</td>\n",
       "      <td>0.937726</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.906454</td>\n",
       "      <td>0.914734</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.603544</td>\n",
       "      <td>0.078022</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0421111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.04211105263157895}</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.924396</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.555460</td>\n",
       "      <td>0.126892</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0473737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.04737368421052632}</td>\n",
       "      <td>0.913106</td>\n",
       "      <td>0.922464</td>\n",
       "      <td>0.893401</td>\n",
       "      <td>0.909662</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.546170</td>\n",
       "      <td>0.120943</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0526363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.05263631578947369}</td>\n",
       "      <td>0.934830</td>\n",
       "      <td>0.902899</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.522139</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0578989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.05789894736842106}</td>\n",
       "      <td>0.926140</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.926033</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>0.012298</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.666219</td>\n",
       "      <td>0.188130</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0631616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.06316157894736842}</td>\n",
       "      <td>0.929037</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>0.931109</td>\n",
       "      <td>0.921981</td>\n",
       "      <td>0.011475</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.585439</td>\n",
       "      <td>0.100038</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0684242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.06842421052631578}</td>\n",
       "      <td>0.929037</td>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.911530</td>\n",
       "      <td>0.920531</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.554711</td>\n",
       "      <td>0.136654</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0736868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.07368684210526316}</td>\n",
       "      <td>0.938450</td>\n",
       "      <td>0.903623</td>\n",
       "      <td>0.910080</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>0.015130</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.605660</td>\n",
       "      <td>0.074120</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0789495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.07894947368421053}</td>\n",
       "      <td>0.939175</td>\n",
       "      <td>0.928261</td>\n",
       "      <td>0.915881</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.510284</td>\n",
       "      <td>0.107721</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0842121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.0842121052631579}</td>\n",
       "      <td>0.921796</td>\n",
       "      <td>0.931159</td>\n",
       "      <td>0.926033</td>\n",
       "      <td>0.926329</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.444160</td>\n",
       "      <td>0.112524</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0894747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.08947473684210526}</td>\n",
       "      <td>0.922520</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.914431</td>\n",
       "      <td>0.920290</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.766581</td>\n",
       "      <td>0.106959</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0947374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.09473736842105263}</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.926759</td>\n",
       "      <td>0.931401</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.519645</td>\n",
       "      <td>0.147170</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.939899</td>\n",
       "      <td>0.902899</td>\n",
       "      <td>0.881798</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.024015</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.648888</td>\n",
       "      <td>0.175184</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'learning_rate_init': 0.001}</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.903623</td>\n",
       "      <td>0.907904</td>\n",
       "      <td>0.916184</td>\n",
       "      <td>0.014832</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.297736</td>\n",
       "      <td>0.096523</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00621053</td>\n",
       "      <td>{'learning_rate_init': 0.006210526315789474}</td>\n",
       "      <td>0.934106</td>\n",
       "      <td>0.915942</td>\n",
       "      <td>0.868745</td>\n",
       "      <td>0.906280</td>\n",
       "      <td>0.027544</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.351950</td>\n",
       "      <td>0.062611</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0114211</td>\n",
       "      <td>{'learning_rate_init': 0.011421052631578946}</td>\n",
       "      <td>0.930485</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.926033</td>\n",
       "      <td>0.930435</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.313576</td>\n",
       "      <td>0.057456</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0166316</td>\n",
       "      <td>{'learning_rate_init': 0.01663157894736842}</td>\n",
       "      <td>0.929037</td>\n",
       "      <td>0.911594</td>\n",
       "      <td>0.918782</td>\n",
       "      <td>0.919807</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.355323</td>\n",
       "      <td>0.154534</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0218421</td>\n",
       "      <td>{'learning_rate_init': 0.021842105263157895}</td>\n",
       "      <td>0.842867</td>\n",
       "      <td>0.922464</td>\n",
       "      <td>0.925308</td>\n",
       "      <td>0.896860</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.485733</td>\n",
       "      <td>0.073819</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0270526</td>\n",
       "      <td>{'learning_rate_init': 0.02705263157894737}</td>\n",
       "      <td>0.926140</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.905729</td>\n",
       "      <td>0.914976</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.289557</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0322632</td>\n",
       "      <td>{'learning_rate_init': 0.03226315789473684}</td>\n",
       "      <td>0.918899</td>\n",
       "      <td>0.931884</td>\n",
       "      <td>0.923858</td>\n",
       "      <td>0.924879</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.557650</td>\n",
       "      <td>0.311780</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0374737</td>\n",
       "      <td>{'learning_rate_init': 0.03747368421052632}</td>\n",
       "      <td>0.920348</td>\n",
       "      <td>0.931159</td>\n",
       "      <td>0.918782</td>\n",
       "      <td>0.923430</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.463889</td>\n",
       "      <td>0.139435</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0426842</td>\n",
       "      <td>{'learning_rate_init': 0.04268421052631579}</td>\n",
       "      <td>0.931209</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.918841</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.407961</td>\n",
       "      <td>0.063783</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0478947</td>\n",
       "      <td>{'learning_rate_init': 0.04789473684210526}</td>\n",
       "      <td>0.934106</td>\n",
       "      <td>0.892029</td>\n",
       "      <td>0.889050</td>\n",
       "      <td>0.905072</td>\n",
       "      <td>0.020577</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.372176</td>\n",
       "      <td>0.116254</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0531053</td>\n",
       "      <td>{'learning_rate_init': 0.05310526315789474}</td>\n",
       "      <td>0.928313</td>\n",
       "      <td>0.921739</td>\n",
       "      <td>0.911530</td>\n",
       "      <td>0.920531</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.275342</td>\n",
       "      <td>0.042532</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0583158</td>\n",
       "      <td>{'learning_rate_init': 0.05831578947368421}</td>\n",
       "      <td>0.937002</td>\n",
       "      <td>0.901449</td>\n",
       "      <td>0.912255</td>\n",
       "      <td>0.916908</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.307130</td>\n",
       "      <td>0.071267</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0635263</td>\n",
       "      <td>{'learning_rate_init': 0.06352631578947368}</td>\n",
       "      <td>0.941347</td>\n",
       "      <td>0.908696</td>\n",
       "      <td>0.921682</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.375426</td>\n",
       "      <td>0.151534</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0687368</td>\n",
       "      <td>{'learning_rate_init': 0.06873684210526315}</td>\n",
       "      <td>0.889935</td>\n",
       "      <td>0.923913</td>\n",
       "      <td>0.897752</td>\n",
       "      <td>0.903865</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.413894</td>\n",
       "      <td>0.134224</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0739474</td>\n",
       "      <td>{'learning_rate_init': 0.07394736842105264}</td>\n",
       "      <td>0.933382</td>\n",
       "      <td>0.918116</td>\n",
       "      <td>0.923858</td>\n",
       "      <td>0.925121</td>\n",
       "      <td>0.006297</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.250184</td>\n",
       "      <td>0.103027</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0791579</td>\n",
       "      <td>{'learning_rate_init': 0.07915789473684211}</td>\n",
       "      <td>0.910210</td>\n",
       "      <td>0.923188</td>\n",
       "      <td>0.914431</td>\n",
       "      <td>0.915942</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.306375</td>\n",
       "      <td>0.084095</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0843684</td>\n",
       "      <td>{'learning_rate_init': 0.08436842105263158}</td>\n",
       "      <td>0.901521</td>\n",
       "      <td>0.827536</td>\n",
       "      <td>0.910080</td>\n",
       "      <td>0.879710</td>\n",
       "      <td>0.037058</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.388419</td>\n",
       "      <td>0.141281</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0895789</td>\n",
       "      <td>{'learning_rate_init': 0.08957894736842105}</td>\n",
       "      <td>0.929037</td>\n",
       "      <td>0.929710</td>\n",
       "      <td>0.875272</td>\n",
       "      <td>0.911353</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.399661</td>\n",
       "      <td>0.093301</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0947895</td>\n",
       "      <td>{'learning_rate_init': 0.09478947368421052}</td>\n",
       "      <td>0.936278</td>\n",
       "      <td>0.915942</td>\n",
       "      <td>0.899927</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.381734</td>\n",
       "      <td>0.098319</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate_init': 0.1}</td>\n",
       "      <td>0.895728</td>\n",
       "      <td>0.924638</td>\n",
       "      <td>0.922408</td>\n",
       "      <td>0.914251</td>\n",
       "      <td>0.013137</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.777046      0.180831         0.001903        0.000156   \n",
       "1        1.794834      0.610527         0.004020        0.000372   \n",
       "2        0.385262      0.040113         0.001720        0.000206   \n",
       "3        2.016187      0.142113         0.002448        0.000023   \n",
       "4        0.857794      0.007419         0.001642        0.000004   \n",
       "5        0.690793      0.129418         0.001720        0.000146   \n",
       "6        0.563491      0.012447         0.001977        0.000244   \n",
       "7        0.779568      0.267565         0.001972        0.000204   \n",
       "8        0.632248      0.221637         0.001763        0.000060   \n",
       "9        0.782797      0.208850         0.001771        0.000024   \n",
       "10       0.863215      0.300807         0.002014        0.000206   \n",
       "11       0.961452      0.175223         0.002203        0.000102   \n",
       "12       0.780386      0.100554         0.001867        0.000203   \n",
       "13       0.584565      0.096684         0.001928        0.000306   \n",
       "14       0.581026      0.111069         0.001711        0.000014   \n",
       "15       0.603544      0.078022         0.001872        0.000168   \n",
       "16       0.555460      0.126892         0.002377        0.000181   \n",
       "17       0.546170      0.120943         0.001694        0.000003   \n",
       "18       0.522139      0.026607         0.001826        0.000163   \n",
       "19       0.666219      0.188130         0.001804        0.000230   \n",
       "20       0.585439      0.100038         0.001796        0.000095   \n",
       "21       0.554711      0.136654         0.001789        0.000057   \n",
       "22       0.605660      0.074120         0.001733        0.000013   \n",
       "23       0.510284      0.107721         0.001740        0.000012   \n",
       "24       0.444160      0.112524         0.001715        0.000010   \n",
       "25       0.766581      0.106959         0.001726        0.000021   \n",
       "26       0.519645      0.147170         0.001725        0.000012   \n",
       "27       0.648888      0.175184         0.001794        0.000051   \n",
       "28       0.297736      0.096523         0.001729        0.000006   \n",
       "29       0.351950      0.062611         0.001731        0.000015   \n",
       "30       0.313576      0.057456         0.001746        0.000016   \n",
       "31       0.355323      0.154534         0.001706        0.000030   \n",
       "32       0.485733      0.073819         0.001671        0.000007   \n",
       "33       0.289557      0.058979         0.001686        0.000013   \n",
       "34       0.557650      0.311780         0.001661        0.000027   \n",
       "35       0.463889      0.139435         0.001643        0.000007   \n",
       "36       0.407961      0.063783         0.001622        0.000006   \n",
       "37       0.372176      0.116254         0.001783        0.000121   \n",
       "38       0.275342      0.042532         0.001607        0.000012   \n",
       "39       0.307130      0.071267         0.001771        0.000254   \n",
       "40       0.375426      0.151534         0.001775        0.000237   \n",
       "41       0.413894      0.134224         0.001788        0.000268   \n",
       "42       0.250184      0.103027         0.001606        0.000012   \n",
       "43       0.306375      0.084095         0.001592        0.000007   \n",
       "44       0.388419      0.141281         0.001602        0.000009   \n",
       "45       0.399661      0.093301         0.001587        0.000012   \n",
       "46       0.381734      0.098319         0.001785        0.000261   \n",
       "\n",
       "   param_activation param_solver param_alpha param_learning_rate_init  \\\n",
       "0              relu          NaN         NaN                      NaN   \n",
       "1              tanh          NaN         NaN                      NaN   \n",
       "2          identity          NaN         NaN                      NaN   \n",
       "3          logistic          NaN         NaN                      NaN   \n",
       "4               NaN        lbfgs         NaN                      NaN   \n",
       "5               NaN          sgd         NaN                      NaN   \n",
       "6               NaN         adam         NaN                      NaN   \n",
       "7               NaN          NaN       1e-05                      NaN   \n",
       "8               NaN          NaN  0.00527263                      NaN   \n",
       "9               NaN          NaN   0.0105353                      NaN   \n",
       "10              NaN          NaN   0.0157979                      NaN   \n",
       "11              NaN          NaN   0.0210605                      NaN   \n",
       "12              NaN          NaN   0.0263232                      NaN   \n",
       "13              NaN          NaN   0.0315858                      NaN   \n",
       "14              NaN          NaN   0.0368484                      NaN   \n",
       "15              NaN          NaN   0.0421111                      NaN   \n",
       "16              NaN          NaN   0.0473737                      NaN   \n",
       "17              NaN          NaN   0.0526363                      NaN   \n",
       "18              NaN          NaN   0.0578989                      NaN   \n",
       "19              NaN          NaN   0.0631616                      NaN   \n",
       "20              NaN          NaN   0.0684242                      NaN   \n",
       "21              NaN          NaN   0.0736868                      NaN   \n",
       "22              NaN          NaN   0.0789495                      NaN   \n",
       "23              NaN          NaN   0.0842121                      NaN   \n",
       "24              NaN          NaN   0.0894747                      NaN   \n",
       "25              NaN          NaN   0.0947374                      NaN   \n",
       "26              NaN          NaN         0.1                      NaN   \n",
       "27              NaN          NaN         NaN                    0.001   \n",
       "28              NaN          NaN         NaN               0.00621053   \n",
       "29              NaN          NaN         NaN                0.0114211   \n",
       "30              NaN          NaN         NaN                0.0166316   \n",
       "31              NaN          NaN         NaN                0.0218421   \n",
       "32              NaN          NaN         NaN                0.0270526   \n",
       "33              NaN          NaN         NaN                0.0322632   \n",
       "34              NaN          NaN         NaN                0.0374737   \n",
       "35              NaN          NaN         NaN                0.0426842   \n",
       "36              NaN          NaN         NaN                0.0478947   \n",
       "37              NaN          NaN         NaN                0.0531053   \n",
       "38              NaN          NaN         NaN                0.0583158   \n",
       "39              NaN          NaN         NaN                0.0635263   \n",
       "40              NaN          NaN         NaN                0.0687368   \n",
       "41              NaN          NaN         NaN                0.0739474   \n",
       "42              NaN          NaN         NaN                0.0791579   \n",
       "43              NaN          NaN         NaN                0.0843684   \n",
       "44              NaN          NaN         NaN                0.0895789   \n",
       "45              NaN          NaN         NaN                0.0947895   \n",
       "46              NaN          NaN         NaN                      0.1   \n",
       "\n",
       "                                          params  split0_test_score  \\\n",
       "0                         {'activation': 'relu'}           0.938450   \n",
       "1                         {'activation': 'tanh'}           0.940623   \n",
       "2                     {'activation': 'identity'}           0.902969   \n",
       "3                     {'activation': 'logistic'}           0.939175   \n",
       "4                            {'solver': 'lbfgs'}           0.898624   \n",
       "5                              {'solver': 'sgd'}           0.764663   \n",
       "6                             {'solver': 'adam'}           0.893555   \n",
       "7                               {'alpha': 1e-05}           0.912382   \n",
       "8                {'alpha': 0.005272631578947368}           0.921796   \n",
       "9                {'alpha': 0.010535263157894737}           0.945692   \n",
       "10               {'alpha': 0.015797894736842105}           0.935554   \n",
       "11               {'alpha': 0.021060526315789474}           0.940623   \n",
       "12               {'alpha': 0.026323157894736843}           0.939899   \n",
       "13                {'alpha': 0.03158578947368421}           0.932657   \n",
       "14               {'alpha': 0.036848421052631586}           0.937726   \n",
       "15                {'alpha': 0.04211105263157895}           0.940623   \n",
       "16                {'alpha': 0.04737368421052632}           0.913106   \n",
       "17                {'alpha': 0.05263631578947369}           0.934830   \n",
       "18                {'alpha': 0.05789894736842106}           0.926140   \n",
       "19                {'alpha': 0.06316157894736842}           0.929037   \n",
       "20                {'alpha': 0.06842421052631578}           0.929037   \n",
       "21                {'alpha': 0.07368684210526316}           0.938450   \n",
       "22                {'alpha': 0.07894947368421053}           0.939175   \n",
       "23                 {'alpha': 0.0842121052631579}           0.921796   \n",
       "24                {'alpha': 0.08947473684210526}           0.922520   \n",
       "25                {'alpha': 0.09473736842105263}           0.937002   \n",
       "26                                {'alpha': 0.1}           0.939899   \n",
       "27                 {'learning_rate_init': 0.001}           0.937002   \n",
       "28  {'learning_rate_init': 0.006210526315789474}           0.934106   \n",
       "29  {'learning_rate_init': 0.011421052631578946}           0.930485   \n",
       "30   {'learning_rate_init': 0.01663157894736842}           0.929037   \n",
       "31  {'learning_rate_init': 0.021842105263157895}           0.842867   \n",
       "32   {'learning_rate_init': 0.02705263157894737}           0.926140   \n",
       "33   {'learning_rate_init': 0.03226315789473684}           0.918899   \n",
       "34   {'learning_rate_init': 0.03747368421052632}           0.920348   \n",
       "35   {'learning_rate_init': 0.04268421052631579}           0.931209   \n",
       "36   {'learning_rate_init': 0.04789473684210526}           0.934106   \n",
       "37   {'learning_rate_init': 0.05310526315789474}           0.928313   \n",
       "38   {'learning_rate_init': 0.05831578947368421}           0.937002   \n",
       "39   {'learning_rate_init': 0.06352631578947368}           0.941347   \n",
       "40   {'learning_rate_init': 0.06873684210526315}           0.889935   \n",
       "41   {'learning_rate_init': 0.07394736842105264}           0.933382   \n",
       "42   {'learning_rate_init': 0.07915789473684211}           0.910210   \n",
       "43   {'learning_rate_init': 0.08436842105263158}           0.901521   \n",
       "44   {'learning_rate_init': 0.08957894736842105}           0.929037   \n",
       "45   {'learning_rate_init': 0.09478947368421052}           0.936278   \n",
       "46                   {'learning_rate_init': 0.1}           0.895728   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.915217           0.889775         0.914493        0.019878   \n",
       "1            0.929710           0.947063         0.939130        0.007161   \n",
       "2            0.921739           0.865845         0.896860        0.023220   \n",
       "3            0.930435           0.941262         0.936957        0.004690   \n",
       "4            0.898551           0.871646         0.889614        0.012698   \n",
       "5            0.686957           0.754895         0.735507        0.034561   \n",
       "6            0.910870           0.925308         0.909903        0.012981   \n",
       "7            0.916667           0.914431         0.914493        0.001750   \n",
       "8            0.925362           0.918057         0.921739        0.002982   \n",
       "9            0.929710           0.920232         0.931884        0.010507   \n",
       "10           0.889855           0.922408         0.915942        0.019211   \n",
       "11           0.924638           0.899202         0.921498        0.017055   \n",
       "12           0.910145           0.912255         0.920773        0.013559   \n",
       "13           0.930435           0.931835         0.931643        0.000918   \n",
       "14           0.900000           0.906454         0.914734        0.016479   \n",
       "15           0.923913           0.908629         0.924396        0.013066   \n",
       "16           0.922464           0.893401         0.909662        0.012110   \n",
       "17           0.902899           0.912255         0.916667        0.013406   \n",
       "18           0.900000           0.926033         0.917391        0.012298   \n",
       "19           0.905797           0.931109         0.921981        0.011475   \n",
       "20           0.921014           0.911530         0.920531        0.007155   \n",
       "21           0.903623           0.910080         0.917391        0.015130   \n",
       "22           0.928261           0.915881         0.927778        0.009516   \n",
       "23           0.931159           0.926033         0.926329        0.003829   \n",
       "24           0.923913           0.914431         0.920290        0.004180   \n",
       "25           0.930435           0.926759         0.931401        0.004237   \n",
       "26           0.902899           0.881798         0.908213        0.024015   \n",
       "27           0.903623           0.907904         0.916184        0.014832   \n",
       "28           0.915942           0.868745         0.906280        0.027544   \n",
       "29           0.934783           0.926033         0.930435        0.003571   \n",
       "30           0.911594           0.918782         0.919807        0.007159   \n",
       "31           0.922464           0.925308         0.896860        0.038217   \n",
       "32           0.913043           0.905729         0.914976        0.008444   \n",
       "33           0.931884           0.923858         0.924879        0.005351   \n",
       "34           0.931159           0.918782         0.923430        0.005503   \n",
       "35           0.916667           0.908629         0.918841        0.009345   \n",
       "36           0.892029           0.889050         0.905072        0.020577   \n",
       "37           0.921739           0.911530         0.920531        0.006905   \n",
       "38           0.901449           0.912255         0.916908        0.014885   \n",
       "39           0.908696           0.921682         0.923913        0.013425   \n",
       "40           0.923913           0.897752         0.903865        0.014531   \n",
       "41           0.918116           0.923858         0.925121        0.006297   \n",
       "42           0.923188           0.914431         0.915942        0.005406   \n",
       "43           0.827536           0.910080         0.879710        0.037058   \n",
       "44           0.929710           0.875272         0.911353        0.025501   \n",
       "45           0.915942           0.899927         0.917391        0.014875   \n",
       "46           0.924638           0.922408         0.914251        0.013137   \n",
       "\n",
       "    rank_test_score  \n",
       "0                33  \n",
       "1                 1  \n",
       "2                43  \n",
       "3                 2  \n",
       "4                45  \n",
       "5                47  \n",
       "6                37  \n",
       "7                33  \n",
       "8                15  \n",
       "9                 3  \n",
       "10               29  \n",
       "11               16  \n",
       "12               17  \n",
       "13                4  \n",
       "14               32  \n",
       "15               11  \n",
       "16               38  \n",
       "17               27  \n",
       "18               23  \n",
       "19               14  \n",
       "20               18  \n",
       "21               23  \n",
       "22                7  \n",
       "23                8  \n",
       "24               20  \n",
       "25                5  \n",
       "26               39  \n",
       "27               28  \n",
       "28               40  \n",
       "29                6  \n",
       "30               21  \n",
       "31               43  \n",
       "32               31  \n",
       "33               10  \n",
       "34               13  \n",
       "35               22  \n",
       "36               41  \n",
       "37               18  \n",
       "38               26  \n",
       "39               12  \n",
       "40               42  \n",
       "41                9  \n",
       "42               29  \n",
       "43               46  \n",
       "44               36  \n",
       "45               23  \n",
       "46               35  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4140 observations with 57 dimensions\n",
      "Train with approximately 72 epochs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-91b95868402c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#Placeholders for data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Ground_truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Hyper-parameters\"\"\"\n",
    "batch_size = 300            # Batch size for stochastic gradient descent\n",
    "test_size = batch_size      # Temporary heuristic. In future we'd like to decouple testing from batching\n",
    "num_centr = 150             # Number of \"hidden neurons\" that is number of centroids\n",
    "max_iterations = 1000       # Max number of iterations\n",
    "learning_rate = 5e-2        # Learning rate\n",
    "num_classes = 10            # Number of target classes, 10 for MNIST\n",
    "var_rbf = 225               # What variance do you expect workable for the RBF?\n",
    "\n",
    "#Obtain and proclaim sizes\n",
    "N,D = X_train.shape         \n",
    "Ntest = X_test.shape[0]\n",
    "print('We have %s observations with %s dimensions'%(N,D))\n",
    "\n",
    "#Proclaim the epochs\n",
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train with approximately %d epochs' %(epochs))\n",
    "\n",
    "#Placeholders for data\n",
    "x = tf.placeholder('float',shape=[batch_size,D],name='input_data')\n",
    "y_ = tf.placeholder(tf.int64, shape=[batch_size], name = 'Ground_truth')\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Hidden_layer\") as scope:\n",
    "  #Centroids and var are the main trainable parameters of the first layer\n",
    "  centroids = tf.Variable(tf.random_uniform([num_centr,D],dtype=tf.float32),name='centroids')\n",
    "  var = tf.Variable(tf.truncated_normal([num_centr],mean=var_rbf,stddev=5,dtype=tf.float32),name='RBF_variance')\n",
    "  \n",
    "  #For now, we collect the distanc\n",
    "  exp_list = []\n",
    "  for i in xrange(num_centr):\n",
    "        exp_list.append(tf.exp((-1*tf.reduce_sum(tf.square(tf.sub(x,centroids[i,:])),1))/(2*var[i])))\n",
    "        phi = tf.transpose(tf.pack(exp_list))\n",
    "        \n",
    "with tf.name_scope(\"Output_layer\") as scope:\n",
    "    w = tf.Variable(tf.truncated_normal([num_centr,num_classes], stddev=0.1, dtype=tf.float32),name='weight')\n",
    "    bias = tf.Variable( tf.constant(0.1, shape=[num_classes]),name='bias')\n",
    "        \n",
    "    h = tf.matmul(phi,w)+bias\n",
    "    size2 = tf.shape(h)\n",
    "\n",
    "with tf.name_scope(\"Softmax\") as scope:\n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(h,y_)\n",
    "  cost = tf.reduce_sum(loss)\n",
    "  loss_summ = tf.scalar_summary(\"cross entropy_loss\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    tvars = tf.trainable_variables()\n",
    "    #We clip the gradients to prevent explosion\n",
    "    grads = tf.gradients(cost, tvars)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = zip(grads, tvars)\n",
    "    train_step = optimizer.apply_gradients(gradients)\n",
    "#     The following block plots for every trainable variable\n",
    "#      - Histogram of the entries of the Tensor\n",
    "#      - Histogram of the gradient over the Tensor\n",
    "#      - Histogram of the grradient-norm over the Tensor\n",
    "    numel = tf.constant([[0]])\n",
    "    for gradient, variable in gradients:\n",
    "      if isinstance(gradient, ops.IndexedSlices):\n",
    "        grad_values = gradient.values\n",
    "      else:\n",
    "        grad_values = gradient\n",
    "      \n",
    "      numel +=tf.reduce_sum(tf.size(variable))  \n",
    "        \n",
    "      h1 = tf.histogram_summary(variable.name, variable)\n",
    "      h2 = tf.histogram_summary(variable.name + \"/gradients\", grad_values)\n",
    "      h3 = tf.histogram_summary(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "with tf.name_scope(\"Evaluating\") as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(h,1), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "   \n",
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "# For now, we collect performances in a Numpy array.\n",
    "# In future releases, I hope TensorBoard allows for more\n",
    "# flexibility in plotting\n",
    "perf_collect = np.zeros((4,int(np.floor(max_iterations /100))))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  with tf.device(\"/cpu:0\"):\n",
    "    print('Start session')\n",
    "    writer = tf.train.SummaryWriter(\"/home/rob/Dropbox/ml_projects/RBFN_tf/log_tb\", sess.graph_def)\n",
    "\n",
    "    step = 0\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "#    #Debugging\n",
    "#    batch_ind = np.random.choice(N,batch_size,replace=False)\n",
    "#    result = sess.run([phi],feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind]})\n",
    "#    print(result[0])\n",
    "    \n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "      batch_ind = np.random.choice(N,batch_size,replace=False)\n",
    "      if i%100 == 1:\n",
    "        #Measure train performance\n",
    "        result = sess.run([cost,accuracy,train_step],feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind]})\n",
    "        perf_collect[0,step] = result[0]\n",
    "        perf_collect[2,step] = result[1]\n",
    "        \n",
    "        \n",
    "        #Measure test performance\n",
    "        test_ind = np.random.choice(Ntest,test_size,replace=False)\n",
    "        result = sess.run([cost,accuracy,merged], feed_dict={ x: X_test[test_ind], y_: y_test[test_ind]})\n",
    "        perf_collect[1,step] = result[0]\n",
    "        perf_collect[3,step] = result[1]\n",
    "      \n",
    "        #Write information for Tensorboard\n",
    "        summary_str = result[2]\n",
    "        writer.add_summary(summary_str, i)\n",
    "        writer.flush()  #Don't forget this command! It makes sure Python writes the summaries to the log-file\n",
    "        \n",
    "        #Print intermediate numbers to terminal\n",
    "        acc = result[1]\n",
    "        print(\"Estimated accuracy at iteration %s of %s: %s\" % (i,max_iterations, acc))\n",
    "        step += 1\n",
    "      else:\n",
    "        sess.run(train_step,feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind]})\n",
    "        \n",
    "\"\"\"Additional plots\"\"\"\n",
    "plt.figure()\n",
    "plt.plot(perf_collect[2],label = 'Train accuracy')\n",
    "plt.plot(perf_collect[3],label = 'Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(perf_collect[0],label = 'Train cost')\n",
    "plt.plot(perf_collect[1],label = 'Test cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# We can now open TensorBoard. Run the following line from your terminal\n",
    "# tensorboard --logdir=/home/rob/Dropbox/ml_projects/RBFN_tf/log_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-ce893753748a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclip_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
